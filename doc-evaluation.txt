src/evaluation/
---------------------------------------------------------------------------------
1. Accuracy Evaluation Workflow
+--------------------------+
|  Load FP32 and INT8     |
|       Models            |
+-----------+-------------+
            |
            v
+-----------+-------------+
|   Prepare Test Dataset  |
+-----------+-------------+
            |
            v
+-----------+-------------+
| Run Both Models on Test |
|         Data            |
+-----------+-------------+
            |
            v
+-----------+-------------+
| Compute mAP, Precision, |
|        Recall           |
+-----------+-------------+
            |
            v
+-----------+-------------+
| Calculate Accuracy      |
|     Differences         |
+-----------+-------------+
            |
            v
+-----------+-------------+
| Identify Problem Classes|
+-----------+-------------+
            |
            v
+-----------+-------------+
| Generate Accuracy       |
|        Reports          |
+-------------------------+

	- Implementation Details:
		+ Models are evaluated on the same test dataset to ensure fair comparison
		+ Standard detection metrics are calculated for both models:
			-> mAP (mean Average Precision) at different IoU thresholds
			-> Precision, recall, and F1 score for each class
			-> Confusion matrices to understand misclassification patterns
		+ Class-wise accuracy comparison identifies which classes suffer most from 
		quantization
		+ Visualized detection results show the real-world impact of quantization
		
2. Performance Benchmarking Workflow
+-------------------------------+
| Configure Benchmark Parameters|
+---------------+---------------+
                |
                v
+---------------+---------------+
|       Generate Test Inputs    |
+---------------+---------------+
                |
        +-------+--------+
        v                v
+---------------+   +---------------+
| Measure FP32  |   | Measure INT8  |
| Model Perf.   |   | Model Perf.   |
+-------+-------+   +-------+-------+
        |                   |
        v                   v
+-------+-------+   +-------+-------+
| Profile Layer |   | Profile Layer |
| -wise Latency |   | -wise Latency |
+-------+-------+   +-------+-------+
        \               /
         \             /
          v           v
        +---------------+
        | Calculate     |
        | Performance   |
        | Gains         |
        +-------+-------+
                |
                v
    +-----------+-----------+
    | Generate Performance  |
    |        Reports        |
    +-----------------------+

	- Implementation Details:
		+ Performance is measured across different:
			-> Batch sizes to understand throughput scaling
			-> Input resolutions to analyze size sensitivity
			-> Repeated runs to ensure statistical validity
		+ Layer-wise profiling identifies:
			-> Execution hotspots in the models
			-> Layers that benefit most from quantization
			-> Potential bottlenecks in the quantized model
		+ Both latency (time per batch) and throughput (samples per second) 
		are measured
		+ Results are visualized through comparative charts and detailed 
		tables
		
3. Memory Usage Analysis Workflow
+------------------------------+
| Analyze FP32 Model Structure|
+--------------+---------------+
               |
               v
+--------------+---------------+
| Analyze INT8 Model Structure|
+--------------+---------------+

+--------------+---------------+
| Measure FP32 Static Memory  |
+--------------+---------------+
               |
               v
+--------------+---------------+
| Measure FP32 Runtime Memory |
+--------------+---------------+

+--------------+---------------+
| Measure INT8 Static Memory  |
+--------------+---------------+
               |
               v
+--------------+---------------+
| Measure INT8 Runtime Memory |
+--------------+---------------+

         +-----------+-----------+
         |                       |
         v                       v
+----------------+     +----------------+
| FP32 Runtime   |     | INT8 Runtime   |
| Memory Measure |     | Memory Measure |
+-------+--------+     +--------+-------+
         \                   /
          \                 /
           v               v
       +-----------------------+
       |  Calculate Memory     |
       |      Savings          |
       +----------+------------+
                  |
                  v
       +----------+------------+
       | Identify Memory-      |
       | Intensive Layers      |
       +----------+------------+
                  |
                  v
       +----------+------------+
       |  Generate Memory      |
       |       Reports         |
       +-----------------------+

	- Implementation Details:
		+ Memory analysis covers:
			-> Static model size (parameters and buffers)
			-> Runtime memory requirements (including activations)
			-> Layer-wise memory profiling


		+ Detailed breakdown of memory usage by layer type helps 
		identify where the memory is being used
		+ Memory savings are calculated in both absolute (MB) and 
		relative (%) terms
		+ Visualization of memory usage helps understand the 
		benefits of quantization
		
4. Accuracy Drift Analysis Workflow:
+-----------------------------+
|  Load FP32 and INT8 Models |
+-------------+---------------+
              |
              v
+-------------+---------------+
|   Prepare Test Dataset      |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Measure Per-Class Accuracy |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Calculate Accuracy Drift   |
|         Metrics            |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Analyze Model Agreement    |
|         Patterns           |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Identify Critical Layers    |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Test Selective Quantization|
+-------------+---------------+
              |
              v
+-------------+---------------+
| Generate Drift Analysis     |
|         Reports             |
+-----------------------------+

	- Implementation Details:
		+ Accuracy drift is analyzed in detail:
			-> Per-class changes in accuracy
			-> Model agreement statistics 
			(where both models get same/different results)
			-> Identification of which types of objects suffer 
			most from quantization
		+ Critical layer analysis finds which layers contribute 
		most to accuracy loss when quantized
		+ Selective quantization testing evaluates if quantizing 
		only non-critical layers improves results
		+ Visualizations highlight the patterns in accuracy drift
		
5. Model Comparison Workflow:
+-----------------------------+
|  Load FP32 and INT8 Models |
+-------------+---------------+
              |
              v
+-------------+---------------+
|     Prepare Test Inputs     |
+-------------+---------------+
              |
              v
+-------------+---------------+
|   Generate Model Outputs    |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Compare Output Tensors     |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Measure Layer-wise          |
|     Differences             |
+-------------+---------------+
              |
              v
+-------------+---------------+
| Visualize Output Differences|
+-------------+---------------+
              |
              v
+-------------+---------------+
| Generate Comparison Reports |
+-----------------------------+

	- Implementation Details:
		+ Deep model comparison looks at:
			-> Tensor-level differences in model outputs
			-> Differences in intermediate layer activations
			-> Statistical analysis of where quantization causes 
			the largest changes
		+ Visual comparison of detection results shows how differences 
		affect real-world performance
		+ Error analysis correlates output differences with 
		accuracy changes
		+ Reports highlight key areas where the models differ 
		significantly
		
6. Visualization and Reporting Workflow
+-----------------------------+
|  Collect Evaluation Results |
+-------------+---------------+
              |
              v
+-------------+---------------+
|  Generate Summary Metrics   |
+-------------+---------------+
              |
              v
+-------------+------------------------+
| Create Comparative Visualizations   |
+-------------+------------------------+
              |
              v
+-------------+------------------------+
| Generate Detection Visualizations   |
+-------------+------------------------+
              |
              v
+-------------+------------------------+
| Compile Comprehensive Report        |
+-------------+------------------------+
              |
              v
+-------------+------------------------+
| Export in Multiple Formats          |
+-------------+------------------------+
              |
              v
+-------------+------------------------+
| Save Results and Visualizations     |
+-----------------------------+

	- Implementation Details:
		+ Comprehensive reporting includes:
			-> Executive summary with key metrics
			-> Detailed performance comparisons
			-> Visual evidence of model differences
			-> Recommendations for potential improvements
		+ Visualization types include:
			-> Bar charts comparing key metrics
			-> Line charts showing performance trends
			-> Heat maps for confusion matrices
			-> Example detection images showing differences
		+ Reports can be generated in multiple formats (Markdown, HTML, JSON)

----------------------------------------------------------------------------------
Summary of the YOLOv8 QAT Evaluation Implementation

I've implemented a comprehensive evaluation framework for your YOLOv8 
Quantization-Aware Training project that will allow you to thoroughly 
analyze the performance of your quantized models.

Main Components
	- metrics.py: Implements accuracy metrics (mAP, precision, recall) 
	for object detection models
	- visualization.py: Creates visualizations of model performance and 
	detection results
	- compare_models.py: Directly compares FP32 and INT8 models to 
	identify differences
	- latency_testing.py: Measures inference speed and throughput
	- accuracy_drift.py: Analyzes how quantization affects model accuracy
	- memory_profiling.py: Profiles memory usage and savings from quantization

Key Features
	- Comprehensive Evaluation: Covers accuracy, performance, and memory aspects
	- Detailed Analysis: Provides layer-by-layer and class-by-class analysis
	- Flexible Usage: Works with any YOLOv8 model variant
	- Rich Visualizations: Makes complex data easy to understand
	- Professional Reports: Generates detailed reports summarizing findings

Data Flow
	- The components work together in a structured way:
	- First, basic metrics are calculated for both FP32 and INT8 models
	- These metrics feed into specialized analyses (accuracy drift, memory usage)
	- The comparison tools directly analyze differences between models
	- Finally, all results are visualized and compiled into comprehensive reports
	
Benefits for Your Project
	- Verify Quantization Quality: Confirm that quantization preserves accuracy
	- Identify Problem Areas: Find specific layers or classes affected by quantization
	- Measure Real-world Benefits: Quantify speed and memory improvements
	- Make Data-driven Decisions: Use detailed metrics to guide optimization
	- Document Results: Generate professional reports of your findings