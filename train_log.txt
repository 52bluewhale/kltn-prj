(venv) PS F:\kltn-prj> python scripts/train_qat.py --model yolov8n.pt --data datasets/vietnam-traffic-sign-detection/dataset.yaml --epochs 5 --batch-size 8 --lr 0.0005 --qconfig sensitive --phased-training --quant-penalty --fuse --eval --export --save-dir models/checkpoints/qat/experiment_2 --export-dir models/exported --device cpu
F:\kltn-prj\venv\lib\site-packages\albumentations\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
2025-05-31 21:29:15,503 - train_qat - INFO - Running safety check for quantization compatibility...
F:\kltn-prj\venv\lib\site-packages\torch\ao\quantization\observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
2025-05-31 21:29:15,507 - train_qat - INFO - ‚úì Basic quantization compatibility check passed
2025-05-31 21:29:15,516 - train_qat - INFO - üöÄ Starting YOLOv8 QAT Training
2025-05-31 21:29:15,516 - train_qat - INFO - üìÅ Model: yolov8n.pt
2025-05-31 21:29:15,516 - train_qat - INFO - üìä Dataset: datasets/vietnam-traffic-sign-detection/dataset.yaml
2025-05-31 21:29:15,516 - train_qat - INFO - ‚öôÔ∏è QConfig: sensitive
2025-05-31 21:29:15,516 - train_qat - INFO - üîÑ Phased Training: True
2025-05-31 21:29:15,516 - train_qat - INFO - ‚ö° Penalty Loss: True (Œ±=0.01)
2025-05-31 21:29:15,516 - train_qat - INFO - üì¶ Initializing QAT model...
2025-05-31 21:29:15,516 - yolov8_qat - INFO - Loading YOLOv8 model from yolov8n.pt
2025-05-31 21:29:15,657 - train_qat - INFO - ‚öôÔ∏è Preparing model for QAT...
2025-05-31 21:29:15,658 - yolov8_qat - INFO - Preparing model for QAT...
2025-05-31 21:29:15,658 - yolov8_qat - INFO - Setting model to training mode...
2025-05-31 21:29:15,659 - yolov8_qat - INFO - Fusing modules for better quantization...
2025-05-31 21:29:15,661 - yolov8_qat - INFO - Manually applying qconfig to individual modules...
2025-05-31 21:29:15,661 - yolov8_qat - INFO - Applied first layer qconfig to model.0.conv
2025-05-31 21:29:15,662 - yolov8_qat - INFO - Applied qconfig to 64 modules
2025-05-31 21:29:15,664 - yolov8_qat - INFO - Disabled quantization for 53 detection modules
2025-05-31 21:29:15,664 - yolov8_qat - INFO - Applying YOLOv8-specific module handling...
2025-05-31 21:29:15,664 - yolov8_qat - INFO - Found 8 C2f blocks to process
2025-05-31 21:29:15,664 - yolov8_qat - INFO - Applied qconfig to 0 additional submodules in C2f blocks
2025-05-31 21:29:15,666 - yolov8_qat - INFO - QConfig application state:
2025-05-31 21:29:15,666 - yolov8_qat - INFO - Module model.0.conv has qconfig applied
2025-05-31 21:29:15,666 - yolov8_qat - INFO - Module model.1.conv has qconfig applied
2025-05-31 21:29:15,666 - yolov8_qat - INFO - Module model.2.cv1.conv has qconfig applied
2025-05-31 21:29:15,666 - yolov8_qat - INFO - Module model.2.cv2.conv has qconfig applied
2025-05-31 21:29:15,667 - yolov8_qat - INFO - Module model.2.m.0.cv1.conv has qconfig applied
2025-05-31 21:29:15,667 - yolov8_qat - INFO - Total modules: 225, modules with qconfig: 45
2025-05-31 21:29:15,667 - yolov8_qat - INFO - Calling prepare_qat with PyTorch 2.4.1 compatible arguments...
2025-05-31 21:29:15,832 - yolov8_qat - INFO - QAT preparation verified: 135 modules have qconfig applied
2025-05-31 21:29:15,835 - yolov8_qat - INFO - Found 90 FakeQuantize modules after prepare_qat
2025-05-31 21:29:15,838 - train_qat - INFO - üéØ Setting up penalty loss (Œ±=0.01)...
 YOLO model patched with quantization penalty
2025-05-31 21:29:15,841 - yolov8_qat - INFO - ‚úÖ Quantization penalty loss setup complete (alpha=0.01, warmup=5)
üîç Verifying penalty loss integration...
‚úÖ Penalty handler found
‚úÖ Model has penalty patching marker
‚úÖ Forward method is properly patched
‚úÖ Original forward method preserved
‚úÖ Penalty calculation working: 0.000000

üìä Penalty Integration Status: 5/5 checks passed
‚úÖ Penalty loss integration is WORKING
2025-05-31 21:29:15,969 - train_qat - INFO - ‚úÖ Penalty loss integration verified
2025-05-31 21:29:15,969 - train_qat - INFO - ‚úÖ QAT model preparation completed
2025-05-31 21:29:15,969 - train_qat - INFO - üèãÔ∏è Starting QAT training...
2025-05-31 21:29:15,970 - yolov8_qat - INFO - ‚úÖ Using dataset: datasets/vietnam-traffic-sign-detection/dataset.yaml
 Continuous Phased QAT Training Plan:
  Phase 1 (Weight-only): Epochs 1-1
  Phase 2 (Add Activations): Epochs 2-3
  Phase 3 (Full Quantization): Epochs 4-4
  Phase 4 (Fine-tuning): Epochs 5-5
 Penalty loss integration active
2025-05-31 21:29:15,970 - yolov8_qat - INFO - üîß Initializing quantizer state manager...
2025-05-31 21:29:15,970 - src.quantization.quantizer_state_manager - INFO - üîç Building quantizer registry...
2025-05-31 21:29:15,973 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Registry built: 180 quantizers found
2025-05-31 21:29:15,973 - src.quantization.quantizer_state_manager - INFO - ‚öôÔ∏è Setting phase state: phase1_weight_only
2025-05-31 21:29:15,976 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase phase1_weight_only configured:
2025-05-31 21:29:15,976 - src.quantization.quantizer_state_manager - INFO -   - Restored: 0 quantizers
2025-05-31 21:29:15,976 - src.quantization.quantizer_state_manager - INFO -   - Disabled: 135 quantizers
2025-05-31 21:29:15,976 - src.quantization.quantizer_state_manager - INFO -   - Total changes: 135
2025-05-31 21:29:15,976 - src.quantization.quantizer_state_manager - INFO - üìä Verification results:
2025-05-31 21:29:15,977 - src.quantization.quantizer_state_manager - INFO -   - Weight quantizers: 45/45 active
2025-05-31 21:29:15,977 - src.quantization.quantizer_state_manager - INFO -   - Activation quantizers: 0/135 active
2025-05-31 21:29:15,977 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase state verification PASSED
2025-05-31 21:29:15,977 - yolov8_qat - INFO - üéØ Final state for phase1_weight_only:
2025-05-31 21:29:15,977 - yolov8_qat - INFO -   - Weight quantizers: 45/45
2025-05-31 21:29:15,977 - yolov8_qat - INFO -   - Activation quantizers: 0/135
 Starting continuous phased QAT training...
 Dataset confirmed: datasets/vietnam-traffic-sign-detection/dataset.yaml
 Model state before training:
  - Model device: cpu
  - Training mode: True
  - Has qconfig: True
Ultralytics 8.3.146  Python-3.8.10 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i5-11400H 2.70GHz)
engine\trainer: agnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets/vietnam-traffic-sign-detection/dataset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=experiment_2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=False, profile=False, project=models/checkpoints/qat, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=models\checkpoints\qat\experiment_2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None
Overriding model.yaml nc=80 with nc=58

                   from  n    params  module                                       arguments
  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]
  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]
  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]
  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]
  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]
  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]
  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]
  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]
  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]
  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]
 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]
 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]
 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']
 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]
 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]
 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]
 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]
 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]
 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]
 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]
 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]
 22        [15, 18, 21]  1    762622  ultralytics.nn.modules.head.Detect           [58, [64, 128, 256]]
Model summary: 129 layers, 3,022,158 parameters, 3,022,142 gradients, 8.3 GFLOPs

Transferred 319/355 items from pretrained weights
Freezing layer 'model.22.dfl.conv.weight'
train: Fast image access  (ping: 0.10.0 ms, read: 205.960.1 MB/s, size: 74.0 KB)
train: Scanning F:\kltn-prj\datasets\vietnam-traffic-sign-detection\train\labels.cache... 9536 images, 465 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9536/9536 [00:00<?, ?it/s]
albumentations: Blur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))
val: Fast image access  (ping: 0.10.0 ms, read: 190.449.4 MB/s, size: 82.7 KB)
val: Scanning F:\kltn-prj\datasets\vietnam-traffic-sign-detection\valid\labels.cache... 784 images, 169 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 784/784 [00:00<?, ?it/s]
Plotting labels to models\checkpoints\qat\experiment_2\labels.jpg... 
optimizer: 'optimizer=auto' found, ignoring 'lr0=0.0005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... 
optimizer: AdamW(lr=0.000161, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)
Image sizes 640 train, 640 val
Using 0 dataloader workers
Logging results to models\checkpoints\qat\experiment_2
Starting training for 5 epochs...

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        1/5         0G     0.8977      3.966     0.9704         30        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1192/1192 [42:22<00:00,  2.13s/it]
 Penalty Stats - Current: 0.000000, Avg: 0.000000
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:31<00:00,  1.87s/it]
                   all        784       1249      0.508      0.148      0.114     0.0927
 Epoch 1: Transitioning to Phase 2 (Adding Activations)
2025-05-31 22:13:16,799 - src.quantization.quantizer_state_manager - INFO - ‚öôÔ∏è Setting phase state: phase2_activations
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase phase2_activations configured:
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO -   - Restored: 135 quantizers
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO -   - Disabled: 0 quantizers
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO -   - Total changes: 135
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO - üìä Verification results:
2025-05-31 22:13:16,803 - src.quantization.quantizer_state_manager - INFO -   - Weight quantizers: 45/45 active
2025-05-31 22:13:16,804 - src.quantization.quantizer_state_manager - INFO -   - Activation quantizers: 135/135 active
2025-05-31 22:13:16,804 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase state verification PASSED
2025-05-31 22:13:16,804 - yolov8_qat - INFO - üéØ Final state for phase2_activations:
2025-05-31 22:13:16,804 - yolov8_qat - INFO -   - Weight quantizers: 45/45
2025-05-31 22:13:16,804 - yolov8_qat - INFO -   - Activation quantizers: 135/135

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        2/5         0G     0.7824      2.477     0.9286         23        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1192/1192 [42:06<00:00,  2.12s/it]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:19<00:00,  1.63s/it]
                   all        784       1249       0.57      0.256      0.267      0.217

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        3/5         0G     0.7313      1.977     0.9104         22        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1192/1192 [38:33<00:00,  1.94s/it]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:20<00:00,  1.64s/it]
                   all        784       1249       0.62      0.323      0.363      0.294
 Epoch 3: Transitioning to Phase 3 (Full Quantization)
2025-05-31 23:36:37,929 - src.quantization.quantizer_state_manager - INFO - ‚öôÔ∏è Setting phase state: phase3_full_quant
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase phase3_full_quant configured:
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO -   - Restored: 0 quantizers
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO -   - Disabled: 0 quantizers
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO -   - Total changes: 0
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO - üìä Verification results:
2025-05-31 23:36:37,933 - src.quantization.quantizer_state_manager - INFO -   - Weight quantizers: 45/45 active
2025-05-31 23:36:37,935 - src.quantization.quantizer_state_manager - INFO -   - Activation quantizers: 135/135 active
2025-05-31 23:36:37,935 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase state verification PASSED
2025-05-31 23:36:37,935 - yolov8_qat - INFO - üéØ Final state for phase3_full_quant:
2025-05-31 23:36:37,935 - yolov8_qat - INFO -   - Weight quantizers: 45/45
2025-05-31 23:36:37,935 - yolov8_qat - INFO -   - Activation quantizers: 135/135

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        4/5         0G     0.6984      1.691     0.9059         15        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1192/1192 [37:53<00:00,  1.91s/it]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:18<00:00,  1.59s/it]
                   all        784       1249      0.593      0.448      0.456      0.366
 Epoch 4: Transitioning to Phase 4 (Fine-tuning)
2025-06-01 00:15:49,752 - src.quantization.quantizer_state_manager - INFO - ‚öôÔ∏è Setting phase state: phase4_fine_tuning
2025-06-01 00:15:49,753 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase phase4_fine_tuning configured:
2025-06-01 00:15:49,753 - src.quantization.quantizer_state_manager - INFO -   - Restored: 0 quantizers
2025-06-01 00:15:49,753 - src.quantization.quantizer_state_manager - INFO -   - Disabled: 0 quantizers
2025-06-01 00:15:49,753 - src.quantization.quantizer_state_manager - INFO -   - Total changes: 0
2025-06-01 00:15:49,754 - src.quantization.quantizer_state_manager - INFO - üìä Verification results:
2025-06-01 00:15:49,754 - src.quantization.quantizer_state_manager - INFO -   - Weight quantizers: 45/45 active
2025-06-01 00:15:49,754 - src.quantization.quantizer_state_manager - INFO -   - Activation quantizers: 135/135 active
2025-06-01 00:15:49,754 - src.quantization.quantizer_state_manager - INFO - ‚úÖ Phase state verification PASSED
2025-06-01 00:15:49,754 - yolov8_qat - INFO - üéØ Final state for phase4_fine_tuning:
2025-06-01 00:15:49,754 - yolov8_qat - INFO -   - Weight quantizers: 45/45
2025-06-01 00:15:49,754 - yolov8_qat - INFO -   - Activation quantizers: 135/135
 Learning rate reduced for fine-tuning: 6.536600000000001e-06

      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
        5/5         0G     0.6679      1.514     0.8961         18        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1192/1192 [37:42<00:00,  1.90s/it]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:18<00:00,  1.59s/it]
                   all        784       1249      0.672      0.456      0.524      0.427

5 epochs completed in 3.425 hours.
Optimizer stripped from models\checkpoints\qat\experiment_2\weights\last.pt, 6.3MB
Optimizer stripped from models\checkpoints\qat\experiment_2\weights\best.pt, 6.3MB

Validating models\checkpoints\qat\experiment_2\weights\best.pt...
Ultralytics 8.3.146  Python-3.8.10 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i5-11400H 2.70GHz)
Model summary (fused): 72 layers, 3,016,958 parameters, 0 gradients, 8.1 GFLOPs
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [01:00<00:00,  1.23s/it]
                   all        784       1249      0.671      0.456      0.525      0.427
                DP.135          4          4      0.869          1      0.995      0.804
                 P.102         48         50      0.786       0.74      0.769      0.576
                P.103a         10         10          1      0.294      0.486      0.416
                P.103b          1          1          1          0      0.995      0.697
                P.103c         15         18      0.452        0.5      0.526      0.484
                 P.104          9          9       0.43      0.111      0.141      0.121
                P.106a         19         19      0.667      0.842      0.829      0.722
                P.106b         15         23      0.664      0.913      0.919      0.779
                 P.112          4          4      0.263      0.197      0.298      0.243
                 P.115          3          3          1          0      0.115      0.106
                 P.117         21         27      0.613      0.703      0.725      0.548
                P.123a          4          4      0.459       0.25       0.28      0.273
                P.123b         15         18      0.527      0.667      0.673       0.57
                P.124a         16         16      0.498      0.812      0.726      0.581
                P.124c          5          5          1          0     0.0432     0.0383
                 P.125          1          1          0          0          0          0
                 P.127        125        249      0.861      0.863      0.895      0.686
                 P.130        143        171      0.903      0.901      0.931      0.747
                P.131a        150        155      0.868      0.973      0.937      0.773
                 P.137          3          3      0.494      0.329      0.446      0.329
                R.301c          3          3          1          0     0.0574     0.0402
                R.301d          3          3       0.75          1      0.995      0.907
                R.301e          5          5          1      0.357      0.397      0.337
                R.302a         49         49      0.824      0.653      0.759      0.555
                R.302b          6          6      0.259      0.833       0.42      0.373
                 R.303         10         11      0.858      0.818      0.817       0.77
                R.407a         11         11      0.815      0.818      0.849      0.713
                 R.409         10         10      0.888        0.7      0.697      0.548
                 R.425          1          1          0          0      0.249      0.224
                 R.434         17         17      0.231      0.471      0.169      0.109
                S.509a         19         19       0.54      0.474      0.597      0.488
                W.201a         28         30      0.708        0.7      0.698      0.498
                W.201b         13         16      0.184      0.312       0.35      0.251
                W.202a          7          7          1       0.25      0.431       0.33
                W.202b          7          7      0.315      0.205        0.2      0.178
                W.203b          3          3          1          0     0.0616     0.0424
                W.203c          7          7      0.742      0.286      0.437      0.369
                W.205a         12         12      0.669      0.667      0.755      0.589
                W.205b          1          1          1          0      0.995      0.995
                W.205d         15         15          1      0.129      0.392      0.279
                W.207a         13         13      0.404      0.308      0.262       0.21
                W.207b         45         46      0.457      0.761      0.607      0.459
                W.207c         17         17       0.58      0.245      0.314      0.274
                 W.208          9          9          1       0.74      0.975      0.678
                 W.209          3          3      0.278      0.667      0.703      0.572
                 W.210          5          6      0.845      0.333      0.383      0.316
                 W.219          2          2          1          0          0          0
                W.221b          1          1          0          0          0          0
                 W.224         66         66      0.723      0.879      0.877      0.704
                 W.225         15         15      0.396      0.733      0.715      0.623
                 W.227          5          5          1          0     0.0583     0.0438
                 W.233          6          6          1          0     0.0329      0.023
                W.245a         37         37      0.746      0.757      0.819      0.644
Speed: 1.9ms preprocess, 69.2ms inference, 0.0ms loss, 0.7ms postprocess per image
Results saved to models\checkpoints\qat\experiment_2
 Continuous phased QAT training completed successfully
2025-06-01 00:55:54,863 - train_qat - INFO - ‚úÖ QAT training completed successfully
2025-06-01 00:55:54,863 - train_qat - INFO - üíæ Saving QAT model (with FakeQuantize) to: models/checkpoints/qat/experiment_2\qat_model_with_fakequant.pt
2025-06-01 00:55:54,864 - yolov8_qat - INFO - Saving QAT model with quantization preserved to models/checkpoints/qat/experiment_2\qat_model_with_fakequant.pt
2025-06-01 00:55:54,864 - yolov8_qat - INFO - Found 0 FakeQuantize modules to preserve
2025-06-01 00:55:54,897 - yolov8_qat - INFO - Verifying QAT model save...
2025-06-01 00:55:54,949 - yolov8_qat - INFO - ‚úÖ QAT model verification passed
2025-06-01 00:55:54,950 - yolov8_qat - INFO - ‚úÖ QAT model saved successfully: models/checkpoints/qat/experiment_2\qat_model_with_fakequant.pt
2025-06-01 00:55:54,950 - yolov8_qat - INFO - ‚úÖ Quantization preserved: 0 FakeQuantize modules
2025-06-01 00:55:54,950 - train_qat - INFO - ‚úÖ QAT model saved successfully
2025-06-01 00:55:54,950 - train_qat - INFO - üîÑ Converting QAT model to INT8 quantized model...
2025-06-01 00:55:54,950 - yolov8_qat - INFO - Converting QAT model to quantized INT8 model...
2025-06-01 00:55:54,951 - yolov8_qat - INFO - Current quantization status:
2025-06-01 00:55:54,952 - yolov8_qat - INFO -   - FakeQuantize modules: 0
2025-06-01 00:55:54,952 - yolov8_qat - INFO -   - Modules with qconfig: 0
2025-06-01 00:55:54,952 - yolov8_qat - ERROR - ‚ùå Quantization has been lost!
2025-06-01 00:55:54,952 - yolov8_qat - ERROR -    - Model may need to be re-prepared for QAT
2025-06-01 00:55:54,952 - yolov8_qat - ERROR - ‚ùå Cannot convert - QAT structure is missing!
2025-06-01 00:55:54,952 - yolov8_qat - ERROR -    - Model may have been saved/loaded incorrectly
2025-06-01 00:55:54,952 - yolov8_qat - ERROR -    - Try re-preparing the model for QAT
2025-06-01 00:55:54,952 - train_qat - ERROR - ‚ùå INT8 model conversion failed
2025-06-01 00:55:54,952 - train_qat - INFO - üìä Evaluating model...
2025-06-01 00:55:54,952 - yolov8_qat - INFO - Evaluating model on datasets/vietnam-traffic-sign-detection/dataset.yaml...
Ultralytics 8.3.146  Python-3.8.10 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i5-11400H 2.70GHz)
Model summary (fused): 72 layers, 3,016,958 parameters, 0 gradients, 8.1 GFLOPs
val: Fast image access  (ping: 0.10.0 ms, read: 723.0123.3 MB/s, size: 85.3 KB)
val: Scanning F:\kltn-prj\datasets\vietnam-traffic-sign-detection\valid\labels.cache... 784 images, 169 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 784/784 [00:00<?, ?it/s]
                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98/98 [00:52<00:00,  1.87it/s]
                   all        784       1249      0.671      0.456      0.525      0.427
                DP.135          4          4      0.869          1      0.995      0.804
                 P.102         48         50      0.786       0.74      0.769      0.576
                P.103a         10         10          1      0.294      0.486      0.416
                P.103b          1          1          1          0      0.995      0.697
                P.103c         15         18      0.452        0.5      0.526      0.484
                 P.104          9          9       0.43      0.111      0.141      0.121
                P.106a         19         19      0.667      0.842      0.829      0.722
                P.106b         15         23      0.664      0.913      0.919      0.779
                 P.112          4          4      0.263      0.197      0.298      0.243
                 P.115          3          3          1          0      0.115      0.106
                 P.117         21         27      0.613      0.703      0.725      0.548
                P.123a          4          4      0.459       0.25       0.28      0.273
                P.123b         15         18      0.527      0.667      0.673       0.57
                P.124a         16         16      0.498      0.812      0.726      0.581
                P.124c          5          5          1          0     0.0432     0.0383
                 P.125          1          1          0          0          0          0
                 P.127        125        249      0.861      0.863      0.895      0.686
                 P.130        143        171      0.903      0.901      0.931      0.747
                P.131a        150        155      0.868      0.973      0.937      0.773
                 P.137          3          3      0.494      0.329      0.446      0.329
                R.301c          3          3          1          0     0.0574     0.0402
                R.301d          3          3       0.75          1      0.995      0.907
                R.301e          5          5          1      0.357      0.397      0.337
                R.302a         49         49      0.824      0.653      0.759      0.555
                R.302b          6          6      0.259      0.833       0.42      0.373
                 R.303         10         11      0.858      0.818      0.817       0.77
                R.407a         11         11      0.815      0.818      0.849      0.713
                 R.409         10         10      0.888        0.7      0.697      0.548
                 R.425          1          1          0          0      0.249      0.224
                 R.434         17         17      0.231      0.471      0.169      0.109
                S.509a         19         19       0.54      0.474      0.597      0.488
                W.201a         28         30      0.708        0.7      0.698      0.498
                W.201b         13         16      0.184      0.312       0.35      0.251
                W.202a          7          7          1       0.25      0.431       0.33
                W.202b          7          7      0.315      0.205        0.2      0.178
                W.203b          3          3          1          0     0.0616     0.0424
                W.203c          7          7      0.742      0.286      0.437      0.369
                W.205a         12         12      0.669      0.667      0.755      0.589
                W.205b          1          1          1          0      0.995      0.995
                W.205d         15         15          1      0.129      0.392      0.279
                W.207a         13         13      0.404      0.308      0.262       0.21
                W.207b         45         46      0.457      0.761      0.607      0.459
                W.207c         17         17       0.58      0.245      0.314      0.274
                 W.208          9          9          1       0.74      0.975      0.678
                 W.209          3          3      0.278      0.667      0.703      0.572
                 W.210          5          6      0.845      0.333      0.383      0.316
                 W.219          2          2          1          0          0          0
                W.221b          1          1          0          0          0          0
                 W.224         66         66      0.723      0.879      0.877      0.704
                 W.225         15         15      0.396      0.733      0.715      0.623
                 W.227          5          5          1          0     0.0583     0.0438
                 W.233          6          6          1          0     0.0329      0.023
                W.245a         37         37      0.746      0.757      0.819      0.644
Speed: 1.4ms preprocess, 59.8ms inference, 0.0ms loss, 0.6ms postprocess per image
Results saved to models\checkpoints\qat\experiment_2
2025-06-01 00:56:49,771 - train_qat - INFO - üìà Evaluation Results - mAP50: 0.5246, mAP50-95: 0.4271
2025-06-01 00:56:49,771 - train_qat - INFO - üì§ Exporting to onnx...
2025-06-01 00:56:49,772 - yolov8_qat - INFO - Exporting model to onnx format...
Ultralytics 8.3.146  Python-3.8.10 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i5-11400H 2.70GHz)
 ProTip: Export to OpenVINO format for best performance on Intel CPUs. Learn more at https://docs.ultralytics.com/integrations/openvino/

PyTorch: starting from 'models\checkpoints\qat\experiment_2\weights\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 62, 8400) (6.0 MB)

ONNX: starting export with onnx 1.17.0 opset 12...
ONNX: slimming with onnxslim 0.1.53...
ONNX: export success  1.2s, saved as 'models\checkpoints\qat\experiment_2\weights\best.onnx' (11.7 MB)

Export complete (1.3s)
Results saved to F:\kltn-prj\models\checkpoints\qat\experiment_2\weights
Predict:         yolo predict task=detect model=models\checkpoints\qat\experiment_2\weights\best.onnx imgsz=640
Validate:        yolo val task=detect model=models\checkpoints\qat\experiment_2\weights\best.onnx imgsz=640 data=datasets/vietnam-traffic-sign-detection/dataset.yaml
Visualize:       https://netron.app
2025-06-01 00:56:51,103 - yolov8_qat - INFO - Model exported to models\checkpoints\qat\experiment_2\weights\best.onnx
2025-06-01 00:56:51,103 - train_qat - INFO - ‚úÖ Export to onnx completed

================================================================================
üéâ QAT TRAINING COMPLETED SUCCESSFULLY!
================================================================================
üìÅ QAT MODEL (For Continued Training/Fine-tuning):
   Path: models/checkpoints/qat/experiment_2\qat_model_with_fakequant.pt
   Size: 11.84 MB
   Type: QAT with FakeQuantize modules
   Status: ‚úÖ Ready for additional training

üöÄ INT8 QUANTIZED MODEL (For Deployment):
   Status: ‚ùå INT8 model not created

üéØ NEXT STEPS:
   1. ‚ö†Ô∏è Check INT8 conversion issues
   2. üîÑ Try manual conversion if needed
   3. üìä Use QAT model for inference testing
================================================================================
üèÜ TRAINING DIRECTORY: models/checkpoints/qat/experiment_2
================================================================================