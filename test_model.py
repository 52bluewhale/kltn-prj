#!/usr/bin/env python3
"""
IMMEDIATE TEST SCRIPT - Run this right now to test best_qat.pt
Copy and paste this into a new .py file and run it immediately.
"""

import torch
import os
import numpy as np
from ultralytics import YOLO
import traceback

def immediate_diagnosis():
    """Immediate diagnosis of the best_qat.pt model."""
    
    model_path = "F:/kltn-prj/models/checkpoints/qat_standard/best_qat.pt"
    
    print("ğŸš¨ IMMEDIATE DIAGNOSIS: best_qat.pt")
    print("="*50)
    
    # Critical Check 1: File exists and size
    print("1. ğŸ“ FILE CHECK:")
    if not os.path.exists(model_path):
        print("   âŒ CRITICAL: File does not exist!")
        return "MISSING"
    
    file_size_bytes = os.path.getsize(model_path)
    file_size_kb = file_size_bytes / 1024
    file_size_mb = file_size_bytes / (1024 * 1024)
    
    print(f"   ğŸ“Š Size: {file_size_bytes} bytes ({file_size_kb:.1f} KB, {file_size_mb:.2f} MB)")
    
    if file_size_kb < 100:  # Less than 100 KB is definitely wrong
        print("   âŒ CRITICAL: File size is way too small!")
        print("   ğŸ” Expected: ~3-6 MB for YOLO model")
        print("   ğŸ” Got: {:.1f} KB - This indicates corruption".format(file_size_kb))
        return "CORRUPTED"
    elif file_size_mb < 1:
        print("   âš ï¸  WARNING: File size is suspiciously small")
        return "SUSPICIOUS"
    else:
        print("   âœ… File size looks reasonable")
    
    # Critical Check 2: Can load with PyTorch
    print("\n2. ğŸ”„ PYTORCH LOADING:")
    try:
        data = torch.load(model_path, map_location='cpu')
        print("   âœ… PyTorch can load the file")
        print(f"   ğŸ“Š Data type: {type(data)}")
        
        if isinstance(data, dict):
            print(f"   ğŸ“Š Keys: {list(data.keys())}")
        
    except Exception as e:
        print(f"   âŒ CRITICAL: PyTorch cannot load file!")
        print(f"   ğŸ” Error: {e}")
        return "UNLOADABLE"
    
    # Critical Check 3: Can load with YOLO
    print("\n3. ğŸ¤– YOLO LOADING:")
    try:
        model = YOLO(model_path)
        print("   âœ… YOLO can load the model")
    except Exception as e:
        print(f"   âŒ CRITICAL: YOLO cannot load model!")
        print(f"   ğŸ” Error: {e}")
        return "YOLO_INCOMPATIBLE"
    
    # Critical Check 4: Basic inference
    print("\n4. ğŸ§  INFERENCE TEST:")
    try:
        # Create dummy input
        dummy_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
        
        # Try inference
        results = model(dummy_img, verbose=False)
        print("   âœ… Inference works!")
        
        # Check if results make sense
        if results and len(results) > 0:
            result = results[0]
            if hasattr(result, 'boxes'):
                num_detections = len(result.boxes) if result.boxes is not None else 0
                print(f"   ğŸ“Š Detections: {num_detections}")
                print("   âœ… Model produces valid outputs")
                return "WORKING"
            else:
                print("   âš ï¸  No detection boxes in results")
                return "NO_OUTPUTS"
        else:
            print("   âš ï¸  Empty results")
            return "EMPTY_RESULTS"
    
    except Exception as e:
        print(f"   âŒ CRITICAL: Inference failed!")
        print(f"   ğŸ” Error: {e}")
        return "INFERENCE_FAILED"

def print_verdict(status):
    """Print final verdict based on diagnosis."""
    print("\n" + "="*50)
    print("ğŸ¯ FINAL VERDICT")
    print("="*50)
    
    if status == "WORKING":
        print("ğŸ‰ GOOD NEWS: Model is FUNCTIONAL!")
        print("âœ… best_qat.pt can be used for inference")
        print("âœ… Model is deployable")
        print("\nğŸ“‹ Next steps:")
        print("   1. Test with real images from your dataset")
        print("   2. Measure inference speed and accuracy")
        print("   3. Export to ONNX for production deployment")
        
    elif status == "NO_OUTPUTS" or status == "EMPTY_RESULTS":
        print("âš ï¸  PARTIAL SUCCESS: Model loads but has output issues")
        print("ğŸ”§ The model may work but needs investigation")
        print("\nğŸ“‹ Next steps:")
        print("   1. Test with real images")
        print("   2. Check if model was properly trained")
        print("   3. Verify detection thresholds")
        
    elif status == "CORRUPTED":
        print("âŒ BAD NEWS: Model file is CORRUPTED")
        print("ğŸ’€ The 3.342 KB size confirms this is broken")
        print("\nğŸ” What happened:")
        print("   - PyTorch quantization conversion failed")
        print("   - File save process was incomplete")
        print("   - Model data was lost during conversion")
        print("\nğŸ› ï¸  How to fix:")
        print("   1. Use your QuantizedYOLOv8 class instead")
        print("   2. Don't use torch.quantization.convert() with YOLO")
        print("   3. Save QAT and INT8 models to different files")
        print("   4. Test model size after saving")
        
    elif status == "MISSING":
        print("âŒ BAD NEWS: Model file not found")
        print("ğŸ” Check if training completed successfully")
        
    elif status == "UNLOADABLE":
        print("âŒ BAD NEWS: File is corrupted or invalid format")
        print("ğŸ”§ File exists but contains invalid data")
        
    elif status == "YOLO_INCOMPATIBLE":
        print("âŒ BAD NEWS: Model incompatible with YOLO")
        print("ğŸ”§ Model format doesn't match YOLO expectations")
        
    elif status == "INFERENCE_FAILED":
        print("âŒ BAD NEWS: Model loads but inference fails")
        print("ğŸ”§ Model structure may be broken")
        
    else:
        print("âš ï¸  UNKNOWN STATUS: Unexpected test result")

def main():
    """Main function - run immediate diagnosis."""
    print("ğŸš¨ EMERGENCY DIAGNOSIS OF best_qat.pt")
    print("This will tell you immediately if your model works or not")
    print()
    
    status = immediate_diagnosis()
    print_verdict(status)
    
    print(f"\nğŸ“Š DIAGNOSIS COMPLETE - Status: {status}")

if __name__ == "__main__":
    # Run the immediate test
    main()