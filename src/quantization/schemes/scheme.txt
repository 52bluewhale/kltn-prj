// __init__.py
from .symmetric import (
    symmetric_quantize,
    symmetric_quantize_per_tensor,
    symmetric_quantize_per_channel,
    get_symmetric_qparams,
    create_symmetric_quantization_config,
)

from .asymmetric import (
    asymmetric_quantize,
    asymmetric_quantize_per_tensor,
    asymmetric_quantize_per_channel,
    get_asymmetric_qparams,
    create_asymmetric_quantization_config,
)

from .per_tensor import (
    PerTensorQuantizer,
    create_per_tensor_quantizer,
    UINT8_ASYMMETRIC,
    INT8_SYMMETRIC,
    INT8_ASYMMETRIC,
)

from .per_channel import (
    PerChannelQuantizer,
    create_per_channel_quantizer,
    INT8_SYMMETRIC_PER_CHANNEL,
    INT8_ASYMMETRIC_PER_CHANNEL,
    get_default_weight_quantizer,
    get_default_activation_quantizer,
)

# Common factory functions for easy access
def get_weight_quantizer(config):
    """
    Get weight quantizer from configuration.
    
    Args:
        config: Quantization configuration dictionary
        
    Returns:
        Weight quantizer
    """
    # Extract parameters from config
    weight_config = config.get("weight", {})
    bit_width = weight_config.get("bit_width", 8)
    scheme = weight_config.get("scheme", "per_channel")
    symmetric = weight_config.get("symmetric", True)
    
    if scheme == "per_channel":
        return create_per_channel_quantizer(
            bit_width=bit_width,
            symmetric=symmetric,
            is_signed=True,
            ch_axis=0
        )
    else:
        return create_per_tensor_quantizer(
            bit_width=bit_width,
            symmetric=symmetric,
            is_signed=True
        )

def get_activation_quantizer(config):
    """
    Get activation quantizer from configuration.
    
    Args:
        config: Quantization configuration dictionary
        
    Returns:
        Activation quantizer
    """
    # Extract parameters from config
    act_config = config.get("activation", {})
    bit_width = act_config.get("bit_width", 8)
    scheme = act_config.get("scheme", "per_tensor")
    symmetric = act_config.get("symmetric", False)
    
    if scheme == "per_channel":
        return create_per_channel_quantizer(
            bit_width=bit_width,
            symmetric=symmetric,
            is_signed=False,
            ch_axis=0
        )
    else:
        return create_per_tensor_quantizer(
            bit_width=bit_width,
            symmetric=symmetric,
            is_signed=False
        )

// asymmetric.py
# Range-based quantization

import torch
import torch.nn as nn
import torch.nn.functional as F

# Required constants
QUANT_MIN_INT8 = -128
QUANT_MAX_INT8 = 127
QUANT_MIN_UINT8 = 0
QUANT_MAX_UINT8 = 255

def asymmetric_quantize(x, scale, zero_point, quant_min, quant_max):
    """
    Apply asymmetric quantization to tensor.
    
    Args:
        x: Input tensor
        scale: Scale factor
        zero_point: Zero point offset
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    # Quantize
    x_q = torch.round(x / scale + zero_point)
    
    # Clamp
    x_q = torch.clamp(x_q, quant_min, quant_max)
    
    # Dequantize
    x_dq = (x_q - zero_point) * scale
    
    return x_dq

def asymmetric_quantize_per_tensor(x, scale, zero_point, quant_min=QUANT_MIN_UINT8, quant_max=QUANT_MAX_UINT8):
    """
    Apply asymmetric per-tensor quantization.
    
    Args:
        x: Input tensor
        scale: Scale factor
        zero_point: Zero point offset
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    return asymmetric_quantize(x, scale, zero_point, quant_min, quant_max)

def asymmetric_quantize_per_channel(x, scale, zero_point, axis=0, quant_min=QUANT_MIN_UINT8, quant_max=QUANT_MAX_UINT8):
    """
    Apply asymmetric per-channel quantization.
    
    Args:
        x: Input tensor
        scale: Scale factor per channel
        zero_point: Zero point offset per channel
        axis: Channel axis
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    # Get shape for broadcasting
    shape = [1] * x.dim()
    shape[axis] = -1
    
    # Reshape scale and zero_point for broadcasting
    scale_reshaped = scale.reshape(shape)
    zero_point_reshaped = zero_point.reshape(shape)
    
    # Quantize
    x_q = torch.round(x / scale_reshaped + zero_point_reshaped)
    
    # Clamp
    x_q = torch.clamp(x_q, quant_min, quant_max)
    
    # Dequantize
    x_dq = (x_q - zero_point_reshaped) * scale_reshaped
    
    return x_dq

def calculate_qparams_asymmetric(min_val, max_val, quant_min, quant_max):
    """
    Calculate quantization parameters for asymmetric quantization.
    
    Args:
        min_val: Minimum value in tensor
        max_val: Maximum value in tensor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Scale and zero_point
    """
    # Ensure min_val and max_val are different
    if min_val.item() == max_val.item():
        # Handle constant tensor
        if min_val.item() == 0:
            # All zeros tensor
            scale = torch.tensor(1.0)
            zero_point = torch.tensor(0, dtype=torch.int64)
        else:
            # Non-zero constant tensor
            scale = max_val.abs() / (quant_max - quant_min) * 2.0
            zero_point = torch.tensor((quant_min + quant_max) // 2, dtype=torch.int64)
        return scale, zero_point
    
    # Calculate scale
    scale = (max_val - min_val) / (quant_max - quant_min)
    
    # Calculate zero point
    zero_point = quant_min - torch.round(min_val / scale)
    
    # Clamp zero_point
    zero_point = torch.clamp(zero_point, quant_min, quant_max).to(torch.int64)
    
    return scale, zero_point

def get_asymmetric_qparams(min_val, max_val, quant_min=QUANT_MIN_UINT8, quant_max=QUANT_MAX_UINT8):
    """
    Get quantization parameters for asymmetric quantization.
    
    Args:
        min_val: Minimum value in tensor
        max_val: Maximum value in tensor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Scale and zero_point
    """
    return calculate_qparams_asymmetric(min_val, max_val, quant_min, quant_max)

class AsymmetricQuantizationConfig:
    """
    Configuration for asymmetric quantization.
    """
    
    def __init__(self, bit_width=8, is_signed=False, per_channel=False, channel_axis=0):
        """
        Initialize configuration.
        
        Args:
            bit_width: Bit width for quantization
            is_signed: Whether to use signed quantization
            per_channel: Whether to use per-channel quantization
            channel_axis: Channel axis for per-channel quantization
        """
        self.bit_width = bit_width
        self.is_signed = is_signed
        self.per_channel = per_channel
        self.channel_axis = channel_axis
        
        # Calculate quantization range
        if is_signed:
            self.quant_min = -2 ** (bit_width - 1)
            self.quant_max = 2 ** (bit_width - 1) - 1
        else:
            self.quant_min = 0
            self.quant_max = 2 ** bit_width - 1
    
    def quantize(self, x, scale, zero_point):
        """
        Quantize tensor using this configuration.
        
        Args:
            x: Input tensor
            scale: Scale factor
            zero_point: Zero point offset
            
        Returns:
            Quantized tensor
        """
        if self.per_channel:
            return asymmetric_quantize_per_channel(
                x, scale, zero_point, self.channel_axis, self.quant_min, self.quant_max
            )
        else:
            return asymmetric_quantize_per_tensor(
                x, scale, zero_point, self.quant_min, self.quant_max
            )
    
    def calculate_qparams(self, min_val, max_val):
        """
        Calculate quantization parameters.
        
        Args:
            min_val: Minimum value in tensor
            max_val: Maximum value in tensor
            
        Returns:
            Scale and zero_point
        """
        return get_asymmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)

# Factory function to create quantization config
def create_asymmetric_quantization_config(bit_width=8, is_signed=False, per_channel=False, channel_axis=0):
    """
    Create asymmetric quantization configuration.
    
    Args:
        bit_width: Bit width for quantization
        is_signed: Whether to use signed quantization
        per_channel: Whether to use per-channel quantization
        channel_axis: Channel axis for per-channel quantization
        
    Returns:
        AsymmetricQuantizationConfig object
    """
    return AsymmetricQuantizationConfig(bit_width, is_signed, per_channel, channel_axis)

// per_channel.py
# Channel-wise quantization

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.quantization.fake_quantize import FakeQuantize

from .symmetric import symmetric_quantize_per_channel, get_symmetric_qparams
from .asymmetric import asymmetric_quantize_per_channel, get_asymmetric_qparams

class PerChannelQuantizer:
    """
    Quantizer for per-channel quantization.
    """
    
    def __init__(self, bit_width=8, symmetric=True, is_signed=None, ch_axis=0, reduce_range=False):
        """
        Initialize per-channel quantizer.
        
        Args:
            bit_width: Bit width for quantization
            symmetric: Whether to use symmetric quantization
            is_signed: Whether to use signed quantization (if None, determined from symmetric)
            ch_axis: Channel axis for per-channel quantization
            reduce_range: Whether to reduce quantization range for compatibility
        """
        self.bit_width = bit_width
        self.symmetric = symmetric
        self.ch_axis = ch_axis
        
        # Determine if using signed quantization
        if is_signed is None:
            # Default: unsigned for asymmetric, signed for symmetric
            self.is_signed = symmetric
        else:
            self.is_signed = is_signed
        
        self.reduce_range = reduce_range
        
        # Calculate quantization range
        if self.is_signed:
            self.quant_min = -2 ** (bit_width - 1 - (1 if reduce_range else 0))
            self.quant_max = 2 ** (bit_width - 1 - (1 if reduce_range else 0)) - 1
        else:
            self.quant_min = 0
            self.quant_max = 2 ** (bit_width - (1 if reduce_range else 0)) - 1
    
    def quantize(self, x, scale, zero_point=None):
        """
        Quantize tensor per-channel.
        
        Args:
            x: Input tensor
            scale: Scale factor per channel
            zero_point: Zero point offset per channel (for asymmetric quantization)
            
        Returns:
            Quantized tensor
        """
        if self.symmetric:
            return symmetric_quantize_per_channel(x, scale, self.ch_axis, self.quant_min, self.quant_max)
        else:
            if zero_point is None:
                raise ValueError("zero_point must be provided for asymmetric quantization")
            return asymmetric_quantize_per_channel(x, scale, zero_point, self.ch_axis, self.quant_min, self.quant_max)
    
    def calculate_qparams(self, min_val, max_val):
        """
        Calculate quantization parameters per-channel.
        
        Args:
            min_val: Minimum value in tensor per channel
            max_val: Maximum value in tensor per channel
            
        Returns:
            Scale and zero_point per channel
        """
        if self.symmetric:
            return get_symmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)
        else:
            return get_asymmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)
    
    def to_fake_quantize(self, observer_class):
        """
        Create FakeQuantize module with this quantizer's settings.
        
        Args:
            observer_class: Observer class to use
            
        Returns:
            FakeQuantize module
        """
        if self.symmetric:
            qscheme = torch.per_channel_symmetric
        else:
            qscheme = torch.per_channel_affine
        
        dtype = torch.qint8 if self.is_signed else torch.quint8
        
        # Create and return FakeQuantize module
        return FakeQuantize.with_args(
            observer=observer_class,
            quant_min=self.quant_min,
            quant_max=self.quant_max,
            dtype=dtype,
            qscheme=qscheme,
            ch_axis=self.ch_axis,
            reduce_range=self.reduce_range
        )

# Factory function to create per-channel quantizer
def create_per_channel_quantizer(bit_width=8, symmetric=True, is_signed=None, ch_axis=0, reduce_range=False):
    """
    Create per-channel quantizer.
    
    Args:
        bit_width: Bit width for quantization
        symmetric: Whether to use symmetric quantization
        is_signed: Whether to use signed quantization
        ch_axis: Channel axis for per-channel quantization
        reduce_range: Whether to reduce quantization range for compatibility
        
    Returns:
        PerChannelQuantizer object
    """
    return PerChannelQuantizer(bit_width, symmetric, is_signed, ch_axis, reduce_range)

# Create common quantizer configurations
INT8_SYMMETRIC_PER_CHANNEL = create_per_channel_quantizer(bit_width=8, symmetric=True, is_signed=True, ch_axis=0)
INT8_ASYMMETRIC_PER_CHANNEL = create_per_channel_quantizer(bit_width=8, symmetric=False, is_signed=True, ch_axis=0)

# Helper function to get appropriate quantizer for weights or activations
def get_default_weight_quantizer(per_channel=True, ch_axis=0, bit_width=8):
    """
    Get default quantizer for weights.
    Typically uses symmetric per-channel quantization.
    
    Args:
        per_channel: Whether to use per-channel quantization
        ch_axis: Channel axis
        bit_width: Bit width
        
    Returns:
        Weight quantizer
    """
    if per_channel:
        return create_per_channel_quantizer(bit_width=bit_width, symmetric=True, is_signed=True, ch_axis=ch_axis)
    else:
        # Fallback to per-tensor for special cases
        from .per_tensor import create_per_tensor_quantizer
        return create_per_tensor_quantizer(bit_width=bit_width, symmetric=True, is_signed=True)

def get_default_activation_quantizer(per_channel=False, ch_axis=0, bit_width=8):
    """
    Get default quantizer for activations.
    Typically uses asymmetric per-tensor quantization.
    
    Args:
        per_channel: Whether to use per-channel quantization
        ch_axis: Channel axis
        bit_width: Bit width
        
    Returns:
        Activation quantizer
    """
    if per_channel:
        return create_per_channel_quantizer(bit_width=bit_width, symmetric=False, is_signed=False, ch_axis=ch_axis)
    else:
        # Standard per-tensor for activations
        from .per_tensor import create_per_tensor_quantizer
        return create_per_tensor_quantizer(bit_width=bit_width, symmetric=False, is_signed=False)

// per_tensor.py
# Whole tensor quantization

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.quantization.fake_quantize import FakeQuantize

from .symmetric import symmetric_quantize_per_tensor, get_symmetric_qparams
from .asymmetric import asymmetric_quantize_per_tensor, get_asymmetric_qparams

class PerTensorQuantizer:
    """
    Quantizer for per-tensor quantization.
    """
    
    def __init__(self, bit_width=8, symmetric=False, is_signed=None, reduce_range=False):
        """
        Initialize per-tensor quantizer.
        
        Args:
            bit_width: Bit width for quantization
            symmetric: Whether to use symmetric quantization
            is_signed: Whether to use signed quantization (if None, determined from symmetric)
            reduce_range: Whether to reduce quantization range for compatibility
        """
        self.bit_width = bit_width
        self.symmetric = symmetric
        
        # Determine if using signed quantization
        if is_signed is None:
            # Default: unsigned for asymmetric, signed for symmetric
            self.is_signed = symmetric
        else:
            self.is_signed = is_signed
        
        self.reduce_range = reduce_range
        
        # Calculate quantization range
        if self.is_signed:
            self.quant_min = -2 ** (bit_width - 1 - (1 if reduce_range else 0))
            self.quant_max = 2 ** (bit_width - 1 - (1 if reduce_range else 0)) - 1
        else:
            self.quant_min = 0
            self.quant_max = 2 ** (bit_width - (1 if reduce_range else 0)) - 1
    
    def quantize(self, x, scale, zero_point=None):
        """
        Quantize tensor.
        
        Args:
            x: Input tensor
            scale: Scale factor
            zero_point: Zero point offset (for asymmetric quantization)
            
        Returns:
            Quantized tensor
        """
        if self.symmetric:
            return symmetric_quantize_per_tensor(x, scale, self.quant_min, self.quant_max)
        else:
            if zero_point is None:
                raise ValueError("zero_point must be provided for asymmetric quantization")
            return asymmetric_quantize_per_tensor(x, scale, zero_point, self.quant_min, self.quant_max)
    
    def calculate_qparams(self, min_val, max_val):
        """
        Calculate quantization parameters.
        
        Args:
            min_val: Minimum value in tensor
            max_val: Maximum value in tensor
            
        Returns:
            Scale and zero_point
        """
        if self.symmetric:
            return get_symmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)
        else:
            return get_asymmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)
    
    def to_fake_quantize(self, observer_class):
        """
        Create FakeQuantize module with this quantizer's settings.
        
        Args:
            observer_class: Observer class to use
            
        Returns:
            FakeQuantize module
        """
        if self.symmetric:
            qscheme = torch.per_tensor_symmetric
        else:
            qscheme = torch.per_tensor_affine
        
        dtype = torch.qint8 if self.is_signed else torch.quint8
        
        # Create and return FakeQuantize module
        return FakeQuantize.with_args(
            observer=observer_class,
            quant_min=self.quant_min,
            quant_max=self.quant_max,
            dtype=dtype,
            qscheme=qscheme,
            reduce_range=self.reduce_range
        )

# Factory function to create per-tensor quantizer
def create_per_tensor_quantizer(bit_width=8, symmetric=False, is_signed=None, reduce_range=False):
    """
    Create per-tensor quantizer.
    
    Args:
        bit_width: Bit width for quantization
        symmetric: Whether to use symmetric quantization
        is_signed: Whether to use signed quantization (if None, determined from symmetric)
        reduce_range: Whether to reduce quantization range for compatibility
        
    Returns:
        PerTensorQuantizer object
    """
    return PerTensorQuantizer(bit_width, symmetric, is_signed, reduce_range)

# Create common quantizer configurations
UINT8_ASYMMETRIC = create_per_tensor_quantizer(bit_width=8, symmetric=False, is_signed=False)
INT8_SYMMETRIC = create_per_tensor_quantizer(bit_width=8, symmetric=True, is_signed=True)
INT8_ASYMMETRIC = create_per_tensor_quantizer(bit_width=8, symmetric=False, is_signed=True)

// symmetric.py
# Zero-centered quantization

import torch
import torch.nn as nn
import torch.nn.functional as F

# Required constants
QUANT_MIN_INT8 = -128
QUANT_MAX_INT8 = 127
QUANT_MIN_UINT8 = 0
QUANT_MAX_UINT8 = 255

def symmetric_quantize(x, scale, quant_min, quant_max):
    """
    Apply symmetric quantization to tensor.
    
    Args:
        x: Input tensor
        scale: Scale factor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    # Quantize
    x_q = torch.round(x / scale)
    
    # Clamp
    x_q = torch.clamp(x_q, quant_min, quant_max)
    
    # Dequantize
    x_dq = x_q * scale
    
    return x_dq

def symmetric_quantize_per_tensor(x, scale, quant_min=QUANT_MIN_INT8, quant_max=QUANT_MAX_INT8):
    """
    Apply symmetric per-tensor quantization.
    
    Args:
        x: Input tensor
        scale: Scale factor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    return symmetric_quantize(x, scale, quant_min, quant_max)

def symmetric_quantize_per_channel(x, scale, axis=0, quant_min=QUANT_MIN_INT8, quant_max=QUANT_MAX_INT8):
    """
    Apply symmetric per-channel quantization.
    
    Args:
        x: Input tensor
        scale: Scale factor per channel
        axis: Channel axis
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Quantized tensor
    """
    # Get shape for broadcasting
    shape = [1] * x.dim()
    shape[axis] = -1
    
    # Reshape scale for broadcasting
    scale_reshaped = scale.reshape(shape)
    
    # Quantize
    x_q = torch.round(x / scale_reshaped)
    
    # Clamp
    x_q = torch.clamp(x_q, quant_min, quant_max)
    
    # Dequantize
    x_dq = x_q * scale_reshaped
    
    return x_dq

def calculate_scale_symmetric(min_val, max_val, quant_min=QUANT_MIN_INT8, quant_max=QUANT_MAX_INT8):
    """
    Calculate scale for symmetric quantization.
    
    Args:
        min_val: Minimum value in tensor
        max_val: Maximum value in tensor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Scale factor
    """
    # Get max absolute value
    max_abs = torch.max(torch.abs(min_val), torch.abs(max_val))
    
    # Calculate scale
    scale = max_abs / torch.max(torch.abs(torch.tensor(quant_min)), torch.abs(torch.tensor(quant_max)))
    
    return scale

def get_symmetric_qparams(min_val, max_val, quant_min=QUANT_MIN_INT8, quant_max=QUANT_MAX_INT8):
    """
    Get quantization parameters for symmetric quantization.
    
    Args:
        min_val: Minimum value in tensor
        max_val: Maximum value in tensor
        quant_min: Minimum quantized value
        quant_max: Maximum quantized value
        
    Returns:
        Scale and zero_point
    """
    scale = calculate_scale_symmetric(min_val, max_val, quant_min, quant_max)
    zero_point = torch.zeros_like(scale, dtype=torch.int64)
    
    return scale, zero_point

class SymmetricQuantizationConfig:
    """
    Configuration for symmetric quantization.
    """
    
    def __init__(self, bit_width=8, is_signed=True, per_channel=False, channel_axis=0):
        """
        Initialize configuration.
        
        Args:
            bit_width: Bit width for quantization
            is_signed: Whether to use signed quantization
            per_channel: Whether to use per-channel quantization
            channel_axis: Channel axis for per-channel quantization
        """
        self.bit_width = bit_width
        self.is_signed = is_signed
        self.per_channel = per_channel
        self.channel_axis = channel_axis
        
        # Calculate quantization range
        if is_signed:
            self.quant_min = -2 ** (bit_width - 1)
            self.quant_max = 2 ** (bit_width - 1) - 1
        else:
            self.quant_min = 0
            self.quant_max = 2 ** bit_width - 1
    
    def quantize(self, x, scale):
        """
        Quantize tensor using this configuration.
        
        Args:
            x: Input tensor
            scale: Scale factor
            
        Returns:
            Quantized tensor
        """
        if self.per_channel:
            return symmetric_quantize_per_channel(
                x, scale, self.channel_axis, self.quant_min, self.quant_max
            )
        else:
            return symmetric_quantize_per_tensor(
                x, scale, self.quant_min, self.quant_max
            )
    
    def calculate_qparams(self, min_val, max_val):
        """
        Calculate quantization parameters.
        
        Args:
            min_val: Minimum value in tensor
            max_val: Maximum value in tensor
            
        Returns:
            Scale and zero_point
        """
        return get_symmetric_qparams(min_val, max_val, self.quant_min, self.quant_max)

# Factory function to create quantization config
def create_symmetric_quantization_config(bit_width=8, is_signed=True, per_channel=False, channel_axis=0):
    """
    Create symmetric quantization configuration.
    
    Args:
        bit_width: Bit width for quantization
        is_signed: Whether to use signed quantization
        per_channel: Whether to use per-channel quantization
        channel_axis: Channel axis for per-channel quantization
        
    Returns:
        SymmetricQuantizationConfig object
    """
    return SymmetricQuantizationConfig(bit_width, is_signed, per_channel, channel_axis)